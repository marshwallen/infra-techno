## 🍑 Fire-Flyer File System (3FS) 预览

专为**AI训练与推理工作负载**设计的高性能分布式文件系统，旨在解决大规模数据处理中的存储挑战。通过结合现代SSD的高吞吐能力和RDMA网络的低延迟特性，提供共享存储层，简化分布式应用开发。

### 🍒 核心特性
#### 1 高性能与易用性
- **解耦架构**：整合数千块SSD的吞吐能力与数百个存储节点的网络带宽，支持**跨节点透明访问存储资源**，无需关注数据物理位置。
- **强一致性**：采用**CRAQ协议**（链式复制与分片查询），确保数据强一致性，降低应用逻辑复杂度。
- **标准文件接口**：支持传统文件系统接口（如POSIX），无需学习新API；元数据服务基于事务型键值存储（如FoundationDB），实现无状态管理。

#### 2 多样化工作负载支持
- **数据预处理**：高效管理数据分析流水线产生的海量中间数据，支持层次化目录结构。
- **数据加载器**：支持跨计算节点的**随机访问训练样本**，无需预取或混排数据集。
- **检查点机制**：为大规模训练任务提供高吞吐并行检查点功能，提升容错效率。
- **推理优化（KVCache）**：替代昂贵的DRAM缓存，提供高吞吐、大容量的键值缓存，优化LLM推理性能。


### 🍒 性能表现
#### 1 峰值吞吐
- **测试环境**：180个存储节点（每节点配备16块14TB NVMe SSD及双200Gbps InfiniBand网卡）+ 500+客户端节点。
- **结果**：在背景训练流量存在时，**读取吞吐达6.6 TiB/s**，展现超强并发处理能力。

#### 2 大规模排序（GraySort）
- **测试场景**：使用[smallpond](https://github.com/deepseek-ai/smallpond)对110.5 TiB数据进行排序，分两阶段（数据分区与分区内排序）。
- **集群配置**：25存储节点 + 50计算节点（总物理核心数约9,600）。
- **结果**：30分钟内完成排序，**平均吞吐3.66 TiB/min**，验证其对数据密集型任务的高效支持。

#### 3 KVCache性能
- **读吞吐**：峰值达**40 GiB/s**，满足高并发推理需求。
- **垃圾回收（GC）效率**：高IOPS（每秒操作数）保障缓存资源的快速回收与复用。

### 🍒 适用场景
- **大规模分布式训练**：如千卡级GPU集群的并行训练与检查点存储。
- **实时推理服务**：通过KVCache降低延迟，提升吞吐。
- **复杂数据流水线**：高效管理预处理、训练、推理全流程数据。
