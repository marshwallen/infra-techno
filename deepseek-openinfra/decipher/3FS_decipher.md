## Fire-Flyer File System (3FS) 技术详解

---

### 🍒 系统架构设计
3FS的核心设计围绕**高性能分布式存储**展开，面向AI训练与推理场景中的大规模随机读取需求，采用**分层架构**与**RDMA网络优化**实现高吞吐与低延迟。系统由四大组件构成：

1. **集群管理器（Cluster Manager）**  
   通过心跳检测管理元数据服务和存储服务的存活状态，处理成员变更并分发集群配置。采用多主选举机制（基于ZooKeeper/etcd），主节点故障时自动切换新主，保障高可用性。生产环境中复用文件元数据存储（如FoundationDB）减少依赖。

   ```markdown
   🟠 Related Knowledge

   - 多主选举机制（基于ZooKeeper/etcd）: 一种分布式系统中实现高可用性和一致性的核心方法，其核心逻辑是通过共识算法协调多个节点，动态选举主节点或允许并行多主运行

   - ​算法: ZooKeeper -> Zab 协议（复杂，写优化）, etcd -> Raft 协议（简单，易实现）
   ​- 数据模型: ZooKeeper -> 分层文件系统（Znode）, etcd -> 扁平键值存储
   - ​适用场景: ZooKeeper -> 旧系统协调（Kafka）, etcd -> 云原生系统（如 Kubernetes）
   - ​扩展性: ZooKeeper -> 适合写密集型任务, etcd -> 适合读多写少场景

   - Raft 共识算法​: 
        1. ​领导者选举：节点通过超时机制触发选举，成为候选者并向其他节点请求投票，获得多数票后成为 Leader。
        ​2. 日志复制：Leader 将操作日志同步到所有 Follower，确保数据一致性。
        3. ​容错性：奇数节点集群（如 3/5 节点）可容忍最多 (n-1)/2 个节点故障。
   ```

2. **元数据服务（Metadata Service）**  
   无状态设计，依赖事务性键值存储（如FoundationDB）管理文件元数据，包括：
   - **inode结构**：存储文件属性（权限、时间戳）、文件长度、数据块布局（链表、块大小、条带大小）等。
   - **目录项**：通过"DENT"前缀+父inode ID+条目名的键结构实现高效目录遍历。
   - 通过SSI（可串行化快照隔离）事务处理元操作（创建/删除/重命名），支持自动冲突重试。

   ```markdown
   🟠 Related Knowledge

   - 无状态设计: 服务实例不保存会话或请求间的状态信息，每个请求独立处理，依赖外部系统（如事务性键值存储）管理数据 

   - FoundationDB: 事务性键值存储系统，支持强一致性（ACID 事务）和高可用性
        1. 键值存储：仅提供基础的键值操作（如 set\get），不内置关系模型或 SQL 接口。  
        2. 分布式架构：通过分片和复制实现横向扩展，支持 PB 级数据和高并发请求。  
        3. 事务支持：默认提供可串行化隔离级别（SSI），适用于多键原子操作。  
        4. 解耦设计：事务管理、存储引擎和配置系统独立，可灵活适配上层数据模型（如关系型、图数据库等）。  
   ```

3. **存储服务（Storage Service）**  
   每个节点管理多个本地SSD，提供块存储接口，核心特性包括：
   - **链式复制（CRAQ）**：写操作通过链头传播至所有副本，读操作可访问任意副本，结合**全写-任读**模式释放SSD与RDMA带宽。
   - **数据块分片**：文件被均匀切分为固定大小的块（默认200条带），通过轮询与随机打散策略均衡分布到多个SSD链。

   ```markdown
   🟠 Related Knowledge

   - 链式复制（Chain Replication）: 一种基于主从架构的分布式数据复制协议，
   - CRAQ（Chain Replication with Apportioned Queries）: 其改进版本，通过在强一致性基础上优化读取吞吐量
   
   - 结构设计：节点按链式排列，头节点（Head）接收所有写请求并顺序传递至尾节点（Tail）；尾节点处理所有读请求，确保读取的是已提交的最新数据
   - CRAQ的改进与核心机制: 
        1. 每个节点存储对象的多个版本，写操作传播时标记为“脏”（非尾节点）或“清洁”（尾节点），尾节点确认后反向通知链上节点更新为清洁版本。
        2. 读取时，非尾节点若最新版本为“脏”，需向尾节点查询最新提交版本号，确保返回强一致性的数据。

   - 这种设计的作用：
        1. 链式复制：高可用性 + 强一致性 + 负载均摊
        2. 数据块分片：均衡分布和并行处理，突破单节点存储与带宽的限制
   ```

4. **客户端（Client）**  
   提供**FUSE客户端**（易用性优先）与**原生客户端**（性能优先）两种接口。原生客户端通过异步零拷贝API（类似io_uring）优化内存效率，关键结构包括：
   - **Iov**：共享内存区域，支持RDMA直接读写用户态内存，规避内核态拷贝开销。
   - **Ior**：环形缓冲区批量处理I/O请求，通过多线程并行分发至存储服务，减少RPC开销。

   ```markdown
   🟠 Related Knowledge

   - RDMA（远程直接内存访问）: 允许计算机直接访问远程服务器的内存，无需CPU和操作系统介入，实现数据在内存间的零拷贝传输
        ​1. 低延迟：绕过内核协议栈和CPU处理，时延可降至微秒级（传统TCP为毫秒级）
        ​2. 高带宽：通过硬件卸载协议处理，100Gbps网络下CPU占用率仅约10%
        3. ​低CPU开销：数据传输完全由网卡（RNIC）处理，释放CPU资源

   - 零拷贝（Zero-Copy）: 一种优化数据传输效率的技术，其核心目标是减少或消除数据在内存中的冗余拷贝次数，从而降低CPU和内存的开销
        1. 传统数据传输（如文件读取后通过网络发送）: 
        磁盘→内核缓冲区（DMA拷贝）→用户缓冲区（CPU拷贝）→Socket缓冲区（CPU拷贝）→网卡（DMA拷贝）
        整个过程涉及4次上下文切换、4次数据拷贝（2次CPU拷贝、2次DMA拷贝）​，导致CPU资源浪费和延迟增加
        2. ​零拷贝的改进： 直接在内核中完成数据传输；利用DMA（直接内存访问）技术，让硬件设备直接操作内存，减少CPU干预

   - DMA（直接内存访问）: 一种硬件机制，允许DMA控制器绕过CPU直接访问内存

   - FUSE（Filesystem in Userspace）客户端: 一种基于用户态实现文件系统的技术框架，其核心目标是通过用户空间程序替代传统内核模块的开发模式，使开发者能够在不修改内核代码的情况下快速构建自定义文件系统

   - 零拷贝技术和虚拟内存有哪些异同点？
    1. 不同点：
        (1) 零拷贝专注于消除数据复制开销，通过DMA、内存映射等技术，让数据直接在设备与内存或内存之间传输，避免CPU参与冗余拷贝（如传统I/O中的用户态与内核态拷贝）。
        (2) ​虚拟内存解决物理内存不足问题，通过分页机制将磁盘空间虚拟为内存，并实现进程间的内存隔离，使程序认为拥有连续地址空间。
    2. 核心相同点:
        (1) 间接层设计：均通过抽象层优化性能, 零拷贝用内存映射绕过用户态拷贝，虚拟内存用分页机制抽象物理内存
        (2) 硬件协作：依赖DMA（零拷贝）或MMU（虚拟内存）等硬件能力实现高效操作
   ```

---

### 🍒 核心技术实现细节
#### 1 元数据管理优化
- **动态文件长度更新**  
  客户端每5秒上报最大写入位置，元服务通过查询最后一个块的ID和长度确定精确文件长度。针对小文件优化：初始链数16，随写入扩展按指数增长，避免全链扫描。

- **会话管理**  
  仅跟踪写模式打开的文件描述符（fd），防止删除导致数据残留。通过定期清理离线客户端的会话，避免资源泄漏。

#### 2 存储层关键技术
- **CRAQ协议强化**  
  写入时需通过链头串行化操作并锁住数据块，版本号递增保证一致性。故障恢复时：
  - **数据同步**：通过全块替换写（full-chunk-replace）同步离线节点数据。
  - **负载均衡**：采用**平衡不完全区组设计**算法，将故障节点的读流量分散到多个SSD（例如节点A故障时，其余5节点各承担1/5流量）。

- **物理块分配优化**  
  块大小从64KiB到64MiB按2次幂分级，资源池预分配256个物理文件。采用写时复制（COW）与位图管理，减少磁盘碎片：
  - 追加写入时直接修改尾部物理块。
  - 随机写入时分配新块并原子更新元数据。

#### 3 客户端性能突破
- **内存零拷贝**  
  通过RDMA Read直接从网卡读取数据至用户态内存，相比传统文件系统减少5次内存拷贝（内核态↔用户态↔Page Cache），内存带宽占用降低至1/5。

- **对齐优化**  
  客户端自动处理512字节对齐：在用户缓冲区头尾添加填充区域，接收时丢弃无效数据，简化接口使用。

- **批量请求合并**  
  原生客户端将小请求合并为64KB的RDMA操作，实测可最大化网络吞吐。多线程环境下建议使用多个Ior环形缓冲区避免锁竞争。

---

### 🍒 设计权衡与创新点
- **1 FUSE兼容性取舍**  
  保留FUSE接口支持POSIX语义（如原子目录操作、符号链接），但通过**原生客户端旁路内核限制**：FUSE仅处理元操作，数据IO走独立通道，规避其锁竞争瓶颈（实测FUSE仅支持40万次/秒的4KiB读取）。

- **2 强一致性与扩展性平衡**  
  元数据强一致（FoundationDB事务）与存储层最终一致（CRAQ读扩散）结合，既支持跨节点原子操作，又保障海量数据访问的线性扩展。

- **3 硬件资源极致利用**  
  存储节点配置16块15TB SSD+双高速网卡，通过800口InfiniBand交换机连接计算节点，尽管单网卡需共享数据流与训练流量，仍通过RDMA优化实现23.5Gbps读取带宽。

```markdown
🟠 Related Knowledge

- MPI（Message Passing Interface，消息传递接口）: 一种广泛用于并行计算的编程框架，尤其适用于分布式内存系统
    1. ​消息传递模型：MPI通过显式调用通信函数实现进程间数据交换，每个进程拥有独立的内存空间。这种模型支持点对点通信（如MPI_Send/MPI_Recv）和集合通信（如MPI_Bcast广播、MPI_Reduce归约）。
    2. ​通信域与上下文：通过通信域（MPI_COMM_WORLD）和标签（Tags）管理消息的传递范围和类型，确保通信隔离与安全性。
    3. ​标准化与可移植性：MPI是一套国际标准接口（MPI-1、MPI-2等），支持跨平台移植，可在异构网络、超级计算机和集群中运行

- RDMA与MPI的协同优化 (可能的优化方式):
    1. ​零拷贝通信​: MPI + RDMA模式,启用MPI的 MPI_Win_allocate 窗口模式，配合InfiniBand Verbs API（如 ibv_post_send），实现存储节点内存与计算节点GPU显存间的直接数据传输
    2. 通信协议选择: 多rail支持，配置MPI的 ​UCX框架，启用多rail RDMA通道（如双网卡各绑定一个UCX context），实现通信带宽叠加（23.5Gbps → 理论双卡峰值40Gbps）
```

---

### 🍒 典型应用场景
1. **AI训练数据加载**：支持FFRecord格式适配PyTorch DataLoader，实现无缓存随机读取。
2. **检查点存储**：高吞吐并行写入模型参数，规避FUSE单文件写入限制。
3. **推理KV缓存**：替代DRAM缓存，提供更大容量与低成本存储方案。

```markdown
🟠 Related Knowledge

- FFRecord（Fire-Flyer Record）: 专为深度学习训练场景设计的高性能二进制数据存储格式，由幻方AI开发，旨在优化大规模数据集的读取效率，尤其适配其自研的3FS高速文件系统
    1. 合并小文件：FFRecord将海量小样本（如图像、文本）合并为单个或多个大文件（通常≥256MB），减少文件系统频繁开闭小文件的性能损耗。
    2. ​随机批量读取：存储每条数据的偏移量和校验信息，支持按索引随机访问任意批次样本，避免传统顺序读取的局限性。（合并的大文件内部的偏移量）
    3. ​数据校验机制：通过CRC32校验确保数据完整性，防止传输或存储中的损坏。
    4. ​异步I/O支持：基于Linux AIO接口实现异步并发读取，提升吞吐量并降低CPU占用。

- KV Cache（键值缓存）: Transformer架构中用于优化大语言模型（LLM）推理效率的关键技术，其核心是通过缓存历史计算中的Key和Value矩阵，避免自回归生成过程中的重复计算，从而显著提升推理速度并降低资源消耗
    1. 重复计算问题：在自回归生成过程中，模型每次预测新token时需处理所有历史token。若不缓存，每次需重新计算全部Key和Value矩阵，导致计算复杂度为O(n^2)。
    2. ​缓存机制：KV Cache通过存储已处理token的Key和Value，后续生成时仅需计算最新token的Query，并与缓存中的Key、Value进行注意力计算，复杂度降至O(n)。
    3. ​动态管理：缓存随序列长度动态扩展，支持实时调整以适配不同场景需求（如长文本生成或多轮对话）
    4. 缓存命中: 通过键（Key）匹配实现命中，当请求的Key存在于缓存中时，直接返回对应的Value

- 提高KV Cache缓存命中率: 
    1. 高频访问的K-V数据优先缓存
    2. 顺序访问场景下，采用启发式规则预取相邻数据（如检测到连续ChunkID访问时触发异步预取）
    3. 混合使用LRU-K（最近K次使用）和LFU（最不频繁使用）算法，兼顾时间与频率维度
    ...

- 零拷贝技术对于KV Cache的意义: 相当于购买了大容量廉价 DRAM，直接在内存中访问，提升缓存命中率

- 说明: K=XW_k，Key向量存储了该token的特征信息，用于衡量其他token的Query与自身的相关性
- 关于QKV的补充: 
    1. Q并非仅代表“当前已生成句子中最后一个词的特征”，而是 当前正在处理的token的查询需求。在自回归推理中（如生成文本时），每个新token的Q由当前token的信息生成，用于向历史序列“提问”以确定哪些部分需要关注。
    2. K是历史token的特征，用于与Q计算相似度，决定哪些历史信息与当前查询相关
    3. V并不等同于词语本身，而是 经过线性变换的语义特征，代表token的实际内容或信息

- Paged Attention: 针对大模型推理过程中显存管理的创新技术，其核心思想借鉴了操作系统的分页内存管理机制，旨在解决传统 KV Cache 分配方式导致的显存碎片问题
    1. ​显存分块​: 将 KV Cache 划分为固定大小的物理块（Block），每个块存储固定数量 tokens 的键值向量（例如 16 tokens/block）。块之间在物理显存中无需连续，仅通过逻辑块表（Block Table）映射管理。
    2. ​逻辑到物理块的映射​: 每个序列对应一个逻辑块表，记录其 KV Cache 分布在哪些物理块中。例如，一个长序列可能占用多个物理块，而这些块可分散在显存的不同位置。
    3. ​动态分配与复用: 序列生成时逐步申请物理块, 避免预分配冗余空间; 释放的物理块可被其他请求复用

- Flash Attention: 针对 Transformer 模型中自注意力（Self-Attention）机制的优化方法，通过减少显存占用和提升计算效率，解决了传统注意力机制在处理长序列时的性能瓶颈
    1. 分块计算（Tiling）​: 将 Q、K、V 矩阵分割为小块（Block），在 SRAM 中逐块计算局部注意力，避免一次性加载完整矩阵到显存。显存占用从 O(n^2) 降至 O(n)。
    2. 重计算（Recomputation）: 反向传播时不存储中间注意力矩阵，而是通过前向传播保存的 Softmax 归一化因子（如最大值、求和值）重新计算梯度，以计算换显存。
    3. 算子融合（Kernel Fusion）​​: 将矩阵乘法、Softmax、Dropout 等操作融合为单一 GPU 内核（Kernel），减少内存读写次数。
```